{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Time Series Part I</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data\n",
    "\n",
    "**References**:\n",
    "\n",
    "[Iterating through directories with Python](https://stackoverflow.com/questions/19587118/iterating-through-directories-with-python)\n",
    "\n",
    "[Read multiple csv files from multiple folders in Python](https://stackoverflow.com/questions/72663553/read-multiple-csv-files-from-multiple-folders-in-python)\n",
    "\n",
    "[Pandas Doc for read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean1 =  40.6276617954071\n",
      "39.25\n"
     ]
    }
   ],
   "source": [
    "# try and play with it\n",
    "df_b1_1 = pd.read_csv(\"../../data/AReM/bending1/dataset1.csv\", header = 5, names = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23'])\n",
    "df_b1_1\n",
    "# print(df_b1_1.avg_rss12)\n",
    "mean1 = statistics.mean(df_b1_1.avg_rss12)\n",
    "print(\"mean1 = \", mean1)\n",
    "\n",
    "fstQ = statistics.quantiles(df_b1_1.avg_rss12, n=4)\n",
    "print(fstQ[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>avg_rss12</th>\n",
       "      <th>var_rss12</th>\n",
       "      <th>avg_rss13</th>\n",
       "      <th>var_rss13</th>\n",
       "      <th>avg_rss23</th>\n",
       "      <th>var_rss23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250 32.50 0.50 0.00 0.00 18.50 0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500 32.75 0.43 1.00 0.00 18.00 0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750 32.50 0.50 0.00 0.00 17.50 0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000 32.50 0.50 7.50 0.50 17.50 0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1250 32.67 0.47 11.00 1.00 16.75 0.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     time  avg_rss12  var_rss12  avg_rss13  \\\n",
       "0    250 32.50 0.50 0.00 0.00 18.50 0.50         NaN        NaN        NaN   \n",
       "1    500 32.75 0.43 1.00 0.00 18.00 0.00         NaN        NaN        NaN   \n",
       "2    750 32.50 0.50 0.00 0.00 17.50 0.50         NaN        NaN        NaN   \n",
       "3   1000 32.50 0.50 7.50 0.50 17.50 0.87         NaN        NaN        NaN   \n",
       "4  1250 32.67 0.47 11.00 1.00 16.75 0.83         NaN        NaN        NaN   \n",
       "\n",
       "   var_rss13  avg_rss23  var_rss23  \n",
       "0        NaN        NaN        NaN  \n",
       "1        NaN        NaN        NaN  \n",
       "2        NaN        NaN        NaN  \n",
       "3        NaN        NaN        NaN  \n",
       "4        NaN        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b2_4 = pd.read_csv(\"../../data/AReM/bending2/dataset4.csv\", header = 5, names = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23'])\n",
    "df_b2_4.head()  # Here I spotted a major error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>avg_rss12</th>\n",
       "      <th>var_rss12</th>\n",
       "      <th>avg_rss13</th>\n",
       "      <th>var_rss13</th>\n",
       "      <th>avg_rss23</th>\n",
       "      <th>var_rss23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>32.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>32.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>17.50</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1250</td>\n",
       "      <td>32.67</td>\n",
       "      <td>0.47</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.75</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23\n",
       "0   250      32.50       0.50        0.0        0.0      18.50       0.50\n",
       "1   500      32.75       0.43        1.0        0.0      18.00       0.00\n",
       "2   750      32.50       0.50        0.0        0.0      17.50       0.50\n",
       "3  1000      32.50       0.50        7.5        0.5      17.50       0.87\n",
       "4  1250      32.67       0.47       11.0        1.0      16.75       0.83"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After examine the data, I found it's the problem of the separator being tampered with. It should be ',' but now it's a space\n",
    "# Therefore, handle using the following method:\n",
    "df_b2_4 = pd.read_csv(\"../../data/AReM/bending2/dataset4.csv\", header = 5, names = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23'], sep = ' ', index_col = False)\n",
    "df_b2_4.head()  # Now it'a fine\n",
    "\n",
    "# reference:\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23\n",
      "0       250      42.00       0.00      18.00       0.00      11.33       0.94\n",
      "1       500      42.75       0.43      16.75       1.79      18.25       0.43\n",
      "2       750      42.50       0.50      16.75       0.83      19.00       1.22\n",
      "3      1000      43.00       0.82      16.25       0.83      18.00       0.00\n",
      "4      1250      43.67       0.47      12.75       0.83      19.00       0.82\n",
      "..      ...        ...        ...        ...        ...        ...        ...\n",
      "474  118750      44.25       0.83      16.25       1.30      24.00       0.00\n",
      "475  119000      44.00       0.71      11.00       1.00      24.00       0.00\n",
      "476  119250      44.25       0.43      10.25       1.30      22.75       0.83\n",
      "477  119500      43.75       0.43       7.33       2.87      21.50       0.50\n",
      "478  119750      44.50       0.50       3.00       1.22      21.50       0.50\n",
      "\n",
      "[479 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# deprecated\n",
    "# try read files in one single directory\n",
    "path = '../../data/AReM/bending1/'\n",
    "df_list = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    # Check if the file is a CSV (because there are pdf files exist)\n",
    "    if file.endswith('.csv'):\n",
    "        try:\n",
    "            df = pd.read_csv(f'{path}{file}', \n",
    "                             header = 5, \n",
    "                             names = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23'])\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {str(e)}\")   # fail (qaq)\n",
    "\n",
    "print(df_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "88\n",
      "       time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23\n",
      "0       250      42.00       0.00      18.00       0.00      11.33       0.94\n",
      "1       500      42.75       0.43      16.75       1.79      18.25       0.43\n",
      "2       750      42.50       0.50      16.75       0.83      19.00       1.22\n",
      "3      1000      43.00       0.82      16.25       0.83      18.00       0.00\n",
      "4      1250      43.67       0.47      12.75       0.83      19.00       0.82\n",
      "..      ...        ...        ...        ...        ...        ...        ...\n",
      "474  118750      44.25       0.83      16.25       1.30      24.00       0.00\n",
      "475  119000      44.00       0.71      11.00       1.00      24.00       0.00\n",
      "476  119250      44.25       0.43      10.25       1.30      22.75       0.83\n",
      "477  119500      43.75       0.43       7.33       2.87      21.50       0.50\n",
      "478  119750      44.50       0.50       3.00       1.22      21.50       0.50\n",
      "\n",
      "[479 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# reading files in several subdirectories\n",
    "# get all file names\n",
    "rootdir = '../../data/AReM/'\n",
    "all_files = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        # Check if the file is a CSV (because there are pdf files exist)\n",
    "        if file.endswith('.csv'):\n",
    "            all_files.append(os.path.join(subdir, file))\n",
    "\n",
    "print(len(all_files))\n",
    "\n",
    "# reference: https://stackoverflow.com/questions/19587118/iterating-through-directories-with-python\n",
    "\n",
    "# read all 88 data sets\n",
    "df_list = {}    # changed to dictionary for train and test split\n",
    "\n",
    "for file in all_files:\n",
    "    try:\n",
    "        if (file == \"../../data/AReM/bending2/dataset4.csv\"):\n",
    "            df_temp = pd.read_csv(file, \n",
    "                                header = 5, \n",
    "                                names = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23'], \n",
    "                                sep = ' ', \n",
    "                                index_col = False)\n",
    "        else:\n",
    "            df_temp = pd.read_csv(file, \n",
    "                                header = 5, \n",
    "                                names = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23'])\n",
    "        df_list[file] = df_temp\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file}: {str(e)}\")   # fail (qaq)\n",
    "\n",
    "print(len(df_list))\n",
    "print(df_list.get(all_files[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time I run the above code I got the error:\n",
    "```\n",
    "Error reading file ../../data/AReM/cycling/dataset14.csv: Error tokenizing data. C error: Expected 7 fields in line 485, saw 8\n",
    "\n",
    "Error reading file ../../data/AReM/cycling/dataset9.csv: Error tokenizing data. C error: Expected 7 fields in line 485, saw 8\n",
    "```\n",
    "\n",
    "I cleaned the data by deleting the extra comma at the end of these two files\n",
    "\n",
    "Now all 88 instances are collectively stored in `df_list` in their original format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rubrics says: Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1, 2, and 3 in other folders as test data and other datasets as train data.\n",
    "\n",
    "I have to get all the names for the test set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# the names of all the test datasets.\n",
    "rootdir = '../../data/AReM/'\n",
    "test_files = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        # Check if the file is a CSV (because there are pdf files exist)\n",
    "        if (file.endswith('t1.csv') or\n",
    "            file.endswith('t2.csv') or\n",
    "            file.endswith('t3.csv')):\n",
    "            if (not file.endswith(\"bending1/dataset3.csv\")\n",
    "                and not file.endswith(\"bending2/dataset3.csv\")):\n",
    "                test_files.append(os.path.join(subdir, file))\n",
    "\n",
    "test_files.remove(\"../../data/AReM/bending1/dataset3.csv\")\n",
    "test_files.remove(\"../../data/AReM/bending2/dataset3.csv\")\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of test datasets: 19\n",
      "the first test dataset: \n",
      "        time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23\n",
      "0       250      39.25       0.43      23.00       0.00       33.0        0.0\n",
      "1       500      39.25       0.43      23.25       0.43       33.0        0.0\n",
      "2       750      39.50       0.50      23.00       0.71       33.0        0.0\n",
      "3      1000      39.50       0.50      24.00       0.00       33.0        0.0\n",
      "4      1250      39.25       0.43      24.00       0.00       33.0        0.0\n",
      "..      ...        ...        ...        ...        ...        ...        ...\n",
      "474  118750      43.33       0.47      25.00       0.00       30.0        0.0\n",
      "475  119000      43.50       0.50      25.50       0.50       30.0        0.0\n",
      "476  119250      43.50       0.50      24.75       0.43       30.0        0.0\n",
      "477  119500      43.50       0.50      24.33       0.47       30.0        0.0\n",
      "478  119750      43.50       0.50      24.25       0.43       30.0        0.0\n",
      "\n",
      "[479 rows x 7 columns]\n",
      "number of training datasets: 69\n",
      "the first training dataset: \n",
      "        time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23\n",
      "0       250      42.00       0.00      18.00       0.00      11.33       0.94\n",
      "1       500      42.75       0.43      16.75       1.79      18.25       0.43\n",
      "2       750      42.50       0.50      16.75       0.83      19.00       1.22\n",
      "3      1000      43.00       0.82      16.25       0.83      18.00       0.00\n",
      "4      1250      43.67       0.47      12.75       0.83      19.00       0.82\n",
      "..      ...        ...        ...        ...        ...        ...        ...\n",
      "474  118750      44.25       0.83      16.25       1.30      24.00       0.00\n",
      "475  119000      44.00       0.71      11.00       1.00      24.00       0.00\n",
      "476  119250      44.25       0.43      10.25       1.30      22.75       0.83\n",
      "477  119500      43.75       0.43       7.33       2.87      21.50       0.50\n",
      "478  119750      44.50       0.50       3.00       1.22      21.50       0.50\n",
      "\n",
      "[479 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# store datasets used to generate testset into df_test_ori\n",
    "df_test_ori = []\n",
    "for file in test_files:\n",
    "    df_test_ori.append(df_list[file])\n",
    "\n",
    "print(\"number of test datasets:\", len(df_test_ori))\n",
    "print(\"the first test dataset: \\n\", df_test_ori[0])\n",
    "\n",
    "# store training datasets into df_train_ori\n",
    "df_train_ori = []\n",
    "for file in all_files:\n",
    "    if file not in test_files:\n",
    "        df_train_ori.append(df_list[file])\n",
    "\n",
    "print(\"number of training datasets:\", len(df_train_ori))\n",
    "print(\"the first training dataset: \\n\", df_train_ori[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My research starts with the definition of time series and its most used models (like ARMA, ARIMA and VAR). (Reference: [wikipedia: Time Series](https://en.wikipedia.org/wiki/Time_series#:~:text=In%20mathematics%2C%20a%20time%20series,equally%20spaced%20points%20in%20time.) and [wikipedia: Time Domain](https://en.wikipedia.org/wiki/Time_domain#:~:text=Time%20domain%20refers%20to%20the,data%2C%20with%20respect%20to%20time.)) Time Series has basic patterns to be captured in terms of 'trend', 'seasonality', 'cycle' and 'variation', but these are not features what we are looking for here.\n",
    "\n",
    "After getting an idea of how Time Series Analysis is done, I started researching for its Machine Learning strategies. (Reference: [Time Series Forecasting with Machine Learning - youtube.com](https://www.youtube.com/watch?v=_ZQ-lQrK9Rg)) The features being selected usually include: \n",
    "\n",
    "**Simple statistical features:**\n",
    "\n",
    "* Means in each of the 𝑑 dimensions\n",
    "* Standard deviations of the 𝑑 dimensions\n",
    "* Skewness, Kurtosis and Higher order moments of the 𝑑 dimensions\n",
    "* Maximum and Minimum values\n",
    "\n",
    "**Time serie analysis related features:**\n",
    "\n",
    "* The 𝑑×𝑑−1 Cross-Correlations between each dimension and the 𝑑 Auto-Correlations\n",
    "* Orders of the autoregressive (AR), integrated (I) and moving average (MA) part of an estimated ARIMA model\n",
    "* Parameters of the AR part\n",
    "* Parameters of the MA part\n",
    "\n",
    "(Reference: [Stack Exchange: features for time series classification](https://stats.stackexchange.com/questions/50807/features-for-time-series-classification))\n",
    "\n",
    "* Average Amplitude Chang\n",
    "* Approximate Entropy\n",
    "* Autoregressive Coefficients\n",
    "* Box-Counting Dimension\n",
    "* Cepstral Coefficients\n",
    "* Difference absolute stdev value\n",
    "* Detrended fluctation analysis\n",
    "* Higuchi's fractal dimension\n",
    "* Histogram\n",
    "* Integral abs. value\n",
    "* Standard deviation\n",
    "* Kurtosis\n",
    "* Log detector\n",
    "* Modified mean abs val 1\n",
    "* Modified mean abs val 2\n",
    "* Meanabs deviation\n",
    "* Mean abs val. slope\n",
    "* Max. Fractal length\n",
    "* Multiple hamming windows\n",
    "* Multiple trapezoida windows\n",
    "* Myopulse percentage rate\n",
    "* Root mean square\n",
    "* Sample Entropy\n",
    "* Skewness\n",
    "* Slope sign Change\n",
    "* Simple square integral\n",
    "* Absolute temporal moment\n",
    "* Variance\n",
    "* Variance Fracta Dimension\n",
    "* v-order\n",
    "* Willison amplitude\n",
    "* Waveform length\n",
    "* Zero Crossing\n",
    "* Mean\n",
    "(Reference: [Comparison of Different Time and Frequency Domain Feature Extraction Methods on Elbow\n",
    "Gesture’s EMG](https://revistia.org/files/articles/ejis_v2_i3s_16/Cemil.pdf#:~:text=Mean%20is%20the%20most%20common,sample%20length%20of%20the%20signal.&text=Variance%20is%20also%20most%20common%20statistical%20method%20for%20time%20domain%20feature%20extraction.&text=AR%20coefficients%20are%20popular%20feature%20extraction%20method%20for%20biological%20signals.))\n",
    "\n",
    "**Time-domain specifications (TDS)** include:\n",
    "the lower and/or upper bounds of the quantities of the time response such as the first peak time, maximum peak time, rise time, maximum overshoot, maximum undershoot, setting time, and steady-state error.\n",
    "\n",
    "(Reference: [Design of the Second-Order Controller by Time-Domain Objective Functions Using Cuckoo Search  DOI: 10.5772/intechopen.89832](https://www.intechopen.com/chapters/69848))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features used here is as specified in the rubrics in hw3 sheet: \n",
    "\n",
    "minimum, maximum, mean, median, standard deviation, first quartile, and third quartile\n",
    "\n",
    "**References**:\n",
    "\n",
    "[python statistics module Doc](https://docs.python.org/3/library/statistics.html)\n",
    "\n",
    "[How to add one row in an existing Pandas DataFrame?](https://www.geeksforgeeks.org/how-to-add-one-row-in-an-existing-pandas-dataframe/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "min_1_avg_rss12\n"
     ]
    }
   ],
   "source": [
    "# setup the feature names for new dataset we want\n",
    "output_features = []\n",
    "original_features = df_b1_1.columns # get feature names in original datset\n",
    "original_features = original_features.delete(0)\n",
    "# print(original_features)\n",
    "idx = 0\n",
    "for original_feature in original_features:\n",
    "    idx += 1\n",
    "    if original_feature != 'time':\n",
    "        min_name = f'min_{idx}_{original_feature}'\n",
    "        output_features.append(min_name)\n",
    "        max_name = f'max_{idx}_{original_feature}'\n",
    "        output_features.append(max_name)\n",
    "        mean_name = f'mean_{idx}_{original_feature}'\n",
    "        output_features.append(mean_name)\n",
    "        median_name = f'median_{idx}_{original_feature}'\n",
    "        output_features.append(median_name)\n",
    "        std_name = f'std_{idx}_{original_feature}'\n",
    "        output_features.append(std_name)\n",
    "        fstQ_name = f'fstQ_{idx}_{original_feature}'\n",
    "        output_features.append(fstQ_name)\n",
    "        trdQ_name = f'trdQ_{idx}_{original_feature}'\n",
    "        output_features.append(trdQ_name)\n",
    "\n",
    "print(len(output_features))\n",
    "print(output_features[0])\n",
    "\n",
    "# Note: here I expilicitly write the name of the 6 time series names into the feature names we are going to use, just for easy understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_1_avg_rss12</th>\n",
       "      <th>max_1_avg_rss12</th>\n",
       "      <th>mean_1_avg_rss12</th>\n",
       "      <th>median_1_avg_rss12</th>\n",
       "      <th>std_1_avg_rss12</th>\n",
       "      <th>fstQ_1_avg_rss12</th>\n",
       "      <th>trdQ_1_avg_rss12</th>\n",
       "      <th>min_2_var_rss12</th>\n",
       "      <th>max_2_var_rss12</th>\n",
       "      <th>mean_2_var_rss12</th>\n",
       "      <th>...</th>\n",
       "      <th>std_5_avg_rss23</th>\n",
       "      <th>fstQ_5_avg_rss23</th>\n",
       "      <th>trdQ_5_avg_rss23</th>\n",
       "      <th>min_6_var_rss23</th>\n",
       "      <th>max_6_var_rss23</th>\n",
       "      <th>mean_6_var_rss23</th>\n",
       "      <th>median_6_var_rss23</th>\n",
       "      <th>std_6_var_rss23</th>\n",
       "      <th>fstQ_6_var_rss23</th>\n",
       "      <th>trdQ_6_var_rss23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.25</td>\n",
       "      <td>48.00</td>\n",
       "      <td>43.973236</td>\n",
       "      <td>44.50</td>\n",
       "      <td>1.617545</td>\n",
       "      <td>43.33</td>\n",
       "      <td>44.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.413987</td>\n",
       "      <td>...</td>\n",
       "      <td>3.289399</td>\n",
       "      <td>20.50</td>\n",
       "      <td>23.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.556472</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.487674</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>43.459562</td>\n",
       "      <td>43.25</td>\n",
       "      <td>1.383869</td>\n",
       "      <td>42.50</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.376159</td>\n",
       "      <td>...</td>\n",
       "      <td>2.454845</td>\n",
       "      <td>22.25</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.679102</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.623071</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.00</td>\n",
       "      <td>47.75</td>\n",
       "      <td>42.169061</td>\n",
       "      <td>43.50</td>\n",
       "      <td>3.666929</td>\n",
       "      <td>39.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.696514</td>\n",
       "      <td>...</td>\n",
       "      <td>3.853317</td>\n",
       "      <td>30.33</td>\n",
       "      <td>36.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.612088</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.523923</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.00</td>\n",
       "      <td>45.75</td>\n",
       "      <td>41.672526</td>\n",
       "      <td>41.75</td>\n",
       "      <td>2.242551</td>\n",
       "      <td>41.33</td>\n",
       "      <td>42.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.536117</td>\n",
       "      <td>...</td>\n",
       "      <td>2.413538</td>\n",
       "      <td>28.33</td>\n",
       "      <td>31.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.384092</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.389176</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.00</td>\n",
       "      <td>47.40</td>\n",
       "      <td>43.958580</td>\n",
       "      <td>44.33</td>\n",
       "      <td>1.557897</td>\n",
       "      <td>43.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.425658</td>\n",
       "      <td>...</td>\n",
       "      <td>1.985273</td>\n",
       "      <td>35.40</td>\n",
       "      <td>36.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.494322</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.513547</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_1_avg_rss12  max_1_avg_rss12  mean_1_avg_rss12  median_1_avg_rss12  \\\n",
       "0            36.25            48.00         43.973236               44.50   \n",
       "1            37.00            48.00         43.459562               43.25   \n",
       "2            33.00            47.75         42.169061               43.50   \n",
       "3            33.00            45.75         41.672526               41.75   \n",
       "4            35.00            47.40         43.958580               44.33   \n",
       "\n",
       "   std_1_avg_rss12  fstQ_1_avg_rss12  trdQ_1_avg_rss12  min_2_var_rss12  \\\n",
       "0         1.617545             43.33             44.67              0.0   \n",
       "1         1.383869             42.50             45.00              0.0   \n",
       "2         3.666929             39.00             45.00              0.0   \n",
       "3         2.242551             41.33             42.75              0.0   \n",
       "4         1.557897             43.00             45.00              0.0   \n",
       "\n",
       "   max_2_var_rss12  mean_2_var_rss12  ...  std_5_avg_rss23  fstQ_5_avg_rss23  \\\n",
       "0             1.50          0.413987  ...         3.289399             20.50   \n",
       "1             1.58          0.376159  ...         2.454845             22.25   \n",
       "2             3.00          0.696514  ...         3.853317             30.33   \n",
       "3             2.83          0.536117  ...         2.413538             28.33   \n",
       "4             1.70          0.425658  ...         1.985273             35.40   \n",
       "\n",
       "   trdQ_5_avg_rss23  min_6_var_rss23  max_6_var_rss23  mean_6_var_rss23  \\\n",
       "0             23.75              0.0             2.96          0.556472   \n",
       "1             24.00              0.0             5.26          0.679102   \n",
       "2             36.33              0.0             2.18          0.612088   \n",
       "3             31.25              0.0             1.79          0.384092   \n",
       "4             36.50              0.0             1.79          0.494322   \n",
       "\n",
       "   median_6_var_rss23  std_6_var_rss23  fstQ_6_var_rss23  trdQ_6_var_rss23  \n",
       "0                0.49         0.487674              0.00              0.83  \n",
       "1                0.50         0.623071              0.43              0.87  \n",
       "2                0.50         0.523923              0.00              1.00  \n",
       "3                0.43         0.389176              0.00              0.50  \n",
       "4                0.43         0.513547              0.00              0.94  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_feature_values(df):\n",
    "    data_point = []\n",
    "    for feature in original_features:\n",
    "        series = np.array(df[feature])\n",
    "        series_clean = [x for x in series if not pd.isna(x)]\n",
    "        data_point.append(min(series_clean))\n",
    "        data_point.append(max(series_clean))\n",
    "        # reference: https://docs.python.org/3/library/statistics.html\n",
    "        mean = statistics.mean(series_clean)\n",
    "        data_point.append(mean)\n",
    "        median = statistics.median(series_clean)\n",
    "        data_point.append(median)\n",
    "        stdev = statistics.stdev(series_clean)\n",
    "        data_point.append(stdev)\n",
    "        quartile = statistics.quantiles(series_clean, n=4)\n",
    "        data_point.append(quartile[0])\n",
    "        data_point.append(quartile[2])\n",
    "    \n",
    "    return data_point\n",
    "\n",
    "# # check:\n",
    "# print(\"a dataset: \\n\", df_test_ori[0])\n",
    "# print(\"\\n\\na column = a time series: \\n\", df_test_ori[0].iloc[:, 1])\n",
    "# print(\"\\n\\nlength of a datapoint: \\n\", len(calculate_feature_values(df_test_ori[0])))\n",
    "\n",
    "# create empty dataset with fixed column names\n",
    "# for testset\n",
    "df_test = pd.DataFrame([],  columns = output_features)\n",
    "# loop through all original test dataframes \n",
    "for df_ori in df_test_ori:\n",
    "    series = calculate_feature_values(df_ori)\n",
    "    df_test.loc[len(df_test.index)] = series\n",
    "    # reference: https://www.geeksforgeeks.org/how-to-add-one-row-in-an-existing-pandas-dataframe/\n",
    "\n",
    "# for training set\n",
    "df_train = pd.DataFrame([],  columns = output_features)\n",
    "# loop through all original training dataframes \n",
    "for df_ori in df_train_ori:\n",
    "    series = calculate_feature_values(df_ori)\n",
    "    df_train.loc[len(df_train.index)] = series\n",
    "\n",
    "# check\n",
    "# df_test.head()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "\n",
    "[convert  numpy dtypes to native Python types](https://www.w3resource.com/python-exercises/numpy/basic/numpy-basic-exercise-41.php)\n",
    "\n",
    "[Calculating Confidence Intervals with Bootstrapping](https://towardsdatascience.com/calculating-confidence-interval-with-bootstrapping-872c657c058d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.48899641 9.94758056]\n"
     ]
    }
   ],
   "source": [
    "# reference: https://towardsdatascience.com/calculating-confidence-interval-with-bootstrapping-872c657c058d\n",
    "\n",
    "def bootstrap_ci(data, num_samples=10000, ci=0.90):\n",
    "    # Create an empty array to store replicates\n",
    "    bs_replicates = np.empty(num_samples)\n",
    "\n",
    "    # Create bootstrap replicates as much as num_samples\n",
    "    for i in range(num_samples):\n",
    "        # Create a bootstrap sample\n",
    "        bs_sample = np.random.choice(data,size=len(data))\n",
    "        # Get bootstrap replicate and append to bs_replicates\n",
    "        bs_replicates[i] = np.std(bs_sample)\n",
    "    \n",
    "    half_alpha = (1 - ci) / 2   # alpha / 2 = 0.05 in this case\n",
    "    result = np.percentile(bs_replicates, [half_alpha * 100, (1 - half_alpha) * 100])\n",
    "    return result\n",
    "\n",
    "# check correctness of the function\n",
    "print(bootstrap_ci(df_train[\"min_1_avg_rss12\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation and bootstrap CI for each feature\n",
    "results = []\n",
    "\n",
    "for column in df_train.columns:\n",
    "    std_dev = df_train[column].std().item()  # calculate standard deviation\n",
    "    ci_low, ci_high = bootstrap_ci(df_train[column])\n",
    "    ci_range = ci_high - ci_low\n",
    "    results.append({\n",
    "        'Standard Deviation': round(std_dev, 2),\n",
    "        'CI': round(ci_range.item(), 2),\n",
    "        'CI Lower': round(ci_low.item(), 2),\n",
    "        'CI Upper': round(ci_high.item(), 2),\n",
    "    })\n",
    "\n",
    "# Display the results\n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>CI</th>\n",
       "      <th>CI Lower</th>\n",
       "      <th>CI Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min_1_avg_rss12</th>\n",
       "      <td>8.79</td>\n",
       "      <td>2.45</td>\n",
       "      <td>7.48</td>\n",
       "      <td>9.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_1_avg_rss12</th>\n",
       "      <td>4.43</td>\n",
       "      <td>2.13</td>\n",
       "      <td>3.24</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_1_avg_rss12</th>\n",
       "      <td>4.92</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4.30</td>\n",
       "      <td>5.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_1_avg_rss12</th>\n",
       "      <td>4.96</td>\n",
       "      <td>1.16</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_1_avg_rss12</th>\n",
       "      <td>1.76</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fstQ_1_avg_rss12</th>\n",
       "      <td>5.73</td>\n",
       "      <td>1.04</td>\n",
       "      <td>5.12</td>\n",
       "      <td>6.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trdQ_1_avg_rss12</th>\n",
       "      <td>4.78</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.90</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_2_var_rss12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_2_var_rss12</th>\n",
       "      <td>5.15</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4.63</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_2_var_rss12</th>\n",
       "      <td>1.60</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_2_var_rss12</th>\n",
       "      <td>1.44</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_2_var_rss12</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fstQ_2_var_rss12</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trdQ_2_var_rss12</th>\n",
       "      <td>2.16</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_3_avg_rss13</th>\n",
       "      <td>3.05</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_3_avg_rss13</th>\n",
       "      <td>4.74</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.92</td>\n",
       "      <td>5.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_3_avg_rss13</th>\n",
       "      <td>3.86</td>\n",
       "      <td>1.21</td>\n",
       "      <td>3.17</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_3_avg_rss13</th>\n",
       "      <td>3.84</td>\n",
       "      <td>1.26</td>\n",
       "      <td>3.13</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_3_avg_rss13</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fstQ_3_avg_rss13</th>\n",
       "      <td>4.14</td>\n",
       "      <td>1.21</td>\n",
       "      <td>3.44</td>\n",
       "      <td>4.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trdQ_3_avg_rss13</th>\n",
       "      <td>3.94</td>\n",
       "      <td>1.27</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_4_var_rss13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_4_var_rss13</th>\n",
       "      <td>2.31</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_4_var_rss13</th>\n",
       "      <td>1.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_4_var_rss13</th>\n",
       "      <td>1.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_4_var_rss13</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fstQ_4_var_rss13</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trdQ_4_var_rss13</th>\n",
       "      <td>1.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_5_avg_rss23</th>\n",
       "      <td>5.37</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.48</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_5_avg_rss23</th>\n",
       "      <td>5.45</td>\n",
       "      <td>2.12</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_5_avg_rss23</th>\n",
       "      <td>5.12</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.69</td>\n",
       "      <td>6.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_5_avg_rss23</th>\n",
       "      <td>5.27</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.75</td>\n",
       "      <td>6.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_5_avg_rss23</th>\n",
       "      <td>1.06</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fstQ_5_avg_rss23</th>\n",
       "      <td>5.53</td>\n",
       "      <td>2.71</td>\n",
       "      <td>4.02</td>\n",
       "      <td>6.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trdQ_5_avg_rss23</th>\n",
       "      <td>4.96</td>\n",
       "      <td>2.44</td>\n",
       "      <td>3.60</td>\n",
       "      <td>6.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_6_var_rss23</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_6_var_rss23</th>\n",
       "      <td>2.53</td>\n",
       "      <td>0.61</td>\n",
       "      <td>2.19</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_6_var_rss23</th>\n",
       "      <td>1.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_6_var_rss23</th>\n",
       "      <td>1.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_6_var_rss23</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fstQ_6_var_rss23</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trdQ_6_var_rss23</th>\n",
       "      <td>1.55</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Standard Deviation    CI  CI Lower  CI Upper\n",
       "min_1_avg_rss12                   8.79  2.45      7.48      9.93\n",
       "max_1_avg_rss12                   4.43  2.13      3.24      5.37\n",
       "mean_1_avg_rss12                  4.92  1.08      4.30      5.38\n",
       "median_1_avg_rss12                4.96  1.16      4.29      5.45\n",
       "std_1_avg_rss12                   1.76  0.40      1.54      1.93\n",
       "fstQ_1_avg_rss12                  5.73  1.04      5.12      6.16\n",
       "trdQ_1_avg_rss12                  4.78  1.60      3.90      5.49\n",
       "min_2_var_rss12                   0.00  0.00      0.00      0.00\n",
       "max_2_var_rss12                   5.15  0.86      4.63      5.49\n",
       "mean_2_var_rss12                  1.60  0.33      1.39      1.73\n",
       "median_2_var_rss12                1.44  0.34      1.23      1.57\n",
       "std_2_var_rss12                   0.90  0.15      0.81      0.96\n",
       "fstQ_2_var_rss12                  0.95  0.22      0.81      1.04\n",
       "trdQ_2_var_rss12                  2.16  0.43      1.89      2.32\n",
       "min_3_avg_rss13                   3.05  0.39      2.81      3.20\n",
       "max_3_avg_rss13                   4.74  1.40      3.92      5.31\n",
       "mean_3_avg_rss13                  3.86  1.21      3.17      4.38\n",
       "median_3_avg_rss13                3.84  1.26      3.13      4.39\n",
       "std_3_avg_rss13                   0.99  0.42      0.77      1.19\n",
       "fstQ_3_avg_rss13                  4.14  1.21      3.44      4.65\n",
       "trdQ_3_avg_rss13                  3.94  1.27      3.22      4.48\n",
       "min_4_var_rss13                   0.00  0.00      0.00      0.00\n",
       "max_4_var_rss13                   2.31  0.42      2.06      2.48\n",
       "mean_4_var_rss13                  1.18  0.16      1.07      1.23\n",
       "median_4_var_rss13                1.15  0.16      1.04      1.20\n",
       "std_4_var_rss13                   0.47  0.07      0.43      0.50\n",
       "fstQ_4_var_rss13                  0.84  0.13      0.75      0.88\n",
       "trdQ_4_var_rss13                  1.57  0.21      1.43      1.63\n",
       "min_5_avg_rss23                   5.37  3.32      3.48      6.80\n",
       "max_5_avg_rss23                   5.45  2.12      4.23      6.35\n",
       "mean_5_avg_rss23                  5.12  2.60      3.69      6.29\n",
       "median_5_avg_rss23                5.27  2.68      3.75      6.43\n",
       "std_5_avg_rss23                   1.06  0.48      0.79      1.27\n",
       "fstQ_5_avg_rss23                  5.53  2.71      4.02      6.73\n",
       "trdQ_5_avg_rss23                  4.96  2.44      3.60      6.04\n",
       "min_6_var_rss23                   0.05  0.09      0.00      0.09\n",
       "max_6_var_rss23                   2.53  0.61      2.19      2.80\n",
       "mean_6_var_rss23                  1.17  0.16      1.06      1.23\n",
       "median_6_var_rss23                1.10  0.16      1.00      1.16\n",
       "std_6_var_rss23                   0.52  0.07      0.47      0.55\n",
       "fstQ_6_var_rss23                  0.77  0.13      0.69      0.82\n",
       "trdQ_6_var_rss23                  1.55  0.22      1.40      1.62"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results, index=df_train.columns)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reference**:\n",
    "\n",
    "claudi.ai - chat screenshot [here](./Chat_link.pdf)\n",
    "\n",
    "The generative AI gave me a direction of how to estimate and choose the best features. My own thinking is as follows.\n",
    "\n",
    "We have two main criteria here: standard deviation and confidence interval. \n",
    "\n",
    "Standard deviation ususally refelcts how scattered the data points are, in the scopre of the whole dataset. Bigger in value of standard deviation indicates more differences and major movements of human activities captured by the signals. This might contain information that reflects diversity aspect of the data.\n",
    "\n",
    "To analyse confidence interval, we should consider its definition. If a feature has smaller confidence interval, it means our estimates are more precise.\n",
    "\n",
    "Taking the relation of these two values into consideration, the confidence interval is calculated using the standard error. Therefore, the narrower the width of confidence interval is compared to standard deviation, the more reliable and representive this feature is.\n",
    "\n",
    "We want to find features with bigger standard deviation, and a confidence interval comparatively smaller then its standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the code I wrote to evaluate the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min_cnt        87.156862\n",
       "max_cnt       116.838765\n",
       "mean_cnt       89.416178\n",
       "median_cnt     87.939588\n",
       "std_cnt        39.164550\n",
       "fstQ_cnt       39.164550\n",
       "trdQ_cnt       89.586239\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort features by std(descding) and ci(ascending)\n",
    "results_sorted_by_std = results_df.sort_values(by='Standard Deviation', ascending=False)\n",
    "results_sorted_by_ci = results_df.sort_values(by='CI')\n",
    "\n",
    "# initialize a datafreame to show importance\n",
    "initial = {\"min_cnt\": 0, \"max_cnt\": 0, \"mean_cnt\": 0, \"median_cnt\": 0,\n",
    "        \"std_cnt\": 0, \"fstQ_cnt\": 0, \"trdQ_cnt\": 0}\n",
    "importance_df = pd.DataFrame(initial, index=[0])\n",
    "importance_df\n",
    "\n",
    "# count appearance\n",
    "def count_appearance(top_num):\n",
    "    min_cnt = max_cnt = mean_cnt = median_cnt = std_cnt = fstQ_cnt = trdQ_cnt = 0\n",
    "\n",
    "    # weight = 42 - top_num\n",
    "    weight = 10 / (top_num +1)  # My weighting algorithm\n",
    "\n",
    "    top_std_features = results_sorted_by_std.head(top_num).index.tolist()\n",
    "    top_ci_features = results_sorted_by_ci.head(top_num).index.tolist()\n",
    "    for list in [top_std_features, top_ci_features]:\n",
    "        for feature in list:\n",
    "            if 'min' in feature: min_cnt += weight\n",
    "            elif 'max' in feature: max_cnt += weight\n",
    "            elif 'mean' in feature: mean_cnt += weight\n",
    "            elif 'median' in feature: median_cnt += weight\n",
    "            elif 'std' in feature: std_cnt += weight\n",
    "            elif 'fstQ' in feature: fstQ_cnt += weight\n",
    "            elif 'trdQ' in feature: trdQ_cnt += weight\n",
    "\n",
    "    for feature in results_df.index.tolist():\n",
    "        weight = results_df.loc[feature, 'Standard Deviation'].item() - results_df.loc[feature, 'CI'].item()\n",
    "        if 'min' in feature: min_cnt += weight\n",
    "        elif 'max' in feature: max_cnt += weight\n",
    "        elif 'mean' in feature: mean_cnt += weight\n",
    "        elif 'median' in feature: median_cnt += weight\n",
    "        elif 'std' in feature: std_cnt += weight\n",
    "        elif 'fstQ' in feature: fstQ_cnt += weight\n",
    "        elif 'trdQ' in feature: trdQ_cnt += weight\n",
    "\n",
    "    # result = {\"min_cnt\": min_cnt, \"max_cnt\": max_cnt, \"mean_cnt\": mean_cnt, \"median_cnt\": median_cnt,\n",
    "    #         \"std_cnt\": std_cnt, \"fstQ_cnt\": std_cnt, \"trdQ_cnt\": trdQ_cnt}\n",
    "    result = [min_cnt, max_cnt, mean_cnt, median_cnt, std_cnt, std_cnt, trdQ_cnt]\n",
    "    # print(result)\n",
    "    importance_df.loc[len(importance_df.index)] = result\n",
    "    return result\n",
    "\n",
    "for i in range(6, 42, 6):   # can be modified to see difference\n",
    "    count_appearance(i)\n",
    "\n",
    "importance_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, the best 3 features are **max, trdQ, mean**. By changing the range in the above code, **min** and **median** can also be an alternative to **mean**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.4\n",
    "\n",
    "[Question](./ISLR%203.7.4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "Most cases, we expect the training RSS of the **cubic** reggression to be lower than the linear one. (In rare cases the two can be same depending on the irredducible error of the training set. Almost not possible)\n",
    "\n",
    "**Reason**: \n",
    "\n",
    "Start with the assumption we make here: the true relationship is linear. This means, the linear regression is more likely to show the real underlying relationship. Howerver, because we are comparing the **traing** RSS for the two regression. It is obvious that the cubic regression is more complex and curvy, having the ability to fit better with the given training data. Given the true relationship is linear, the added higher-order terms are unnecessary yet can still reduce training RSS. It usually cause overfitting.\n",
    "\n",
    "There are also rare cases that the training RSS of the linear regression the same with that of the cubic. This only happens when ∊, the irreducible error is very small to almost zero, or they cancle out perfectly so that the RSS for the two just the same.\n",
    "\n",
    "The truth is, the cubic terms being added may be redundant for the test set, but it will work better on the training set because of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Linear Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "Most cases, we expect the test RSS of the **linear** reggression to be lower than the cubic one.\n",
    "\n",
    "**Reason**: \n",
    "\n",
    "Same as above, start with the assumption we make here: the true relationship is linear. The linear regression is more likely to show the real underlying relationship. Here we are comparing the test RSS, which introduces new errors, and therefore better reflects the model's generalization. \n",
    "\n",
    "The cubic regression however, is more sensitive to errors for having added unnecessary terms. Consequently, its test RSS is higher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Not Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: In most cases, the cubic regression will likely have lower RSS in both the training set and test set compared to linear regression. However, the degree of difference may vary.\n",
    "\n",
    "**Reason**: \n",
    "\n",
    "Same as above, start with the assumption we make here: the true relationship is not linear, but more complex, regardless we don't know how far it is from linear. Because the cubic regression is more flexible, it can better reflect the complexity of the underlying patterns that deviate from linearity. \n",
    "\n",
    "If the true relationship is even slightly non-linear , such as quadratic, the cubic regression can do better by finding the coefficient ß2 to fit data more closely. \n",
    "\n",
    "As for the linear model, being constrained to a straight line, cannot adapt to these non-linear patterns. Observe the graph for **explanatory power**, the *unexplained variation* for the linear will be higher than that of the cubic, resulting in higher unexplained variation and thus higher training RSS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Not Linear Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "In most cases, the cubic regression will likely have lower RSS in both the training set and test set compared to linear regression.\n",
    "\n",
    "**Reason**: \n",
    "\n",
    "Though the cubic regression will tend to have lower test RSS, the difference may be less exaggerated than in the training set.If the true relationship is moderately non-linear: The cubic model will generally outperform the linear model on test data, as it can better find the underlying pattern. The linear model just cannot capture non-linear trends, and will lead to consistently higher RSS.\n",
    "\n",
    "If the true relationship is slightly non-linear: The cubic model may still have lower RSS, but the difference could be smaller. In some cases, the RSS values might be similar for both models. This can happen if the cubic model is slightly overfitting the training data, causing its performance on the test set to be closer to that of the linear model.\n",
    "\n",
    "If the true relationship is more far from linear, such as cubic or more complex, the cubic model will likely have significantly lower RSS on both training and test sets, as the linear model simply cannot capture the curvy shape of the data.\n",
    "\n",
    "To conclude, the relative performance on the test set depends on how well the cubic model matches the true underlying relationship, balancing the trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 3.7.3 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ISLR 3.7.5 - Extra Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1. Python: \n",
    "\n",
    "    [Iterating through directories with Python](https://stackoverflow.com/questions/19587118/iterating-through-directories-with-python)\n",
    "\n",
    "    [Read multiple csv files from multiple folders in Python](https://stackoverflow.com/questions/72663553/read-multiple-csv-files-from-multiple-folders-in-python)\n",
    "\n",
    "    [Pandas Doc for read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "\n",
    "    [python statistics module Doc](https://docs.python.org/3/library/statistics.html)\n",
    "\n",
    "    [How to add one row in an existing Pandas DataFrame?](https://www.geeksforgeeks.org/how-to-add-one-row-in-an-existing-pandas-dataframe/)\n",
    "\n",
    "    [convert  numpy dtypes to native Python types](https://www.w3resource.com/python-exercises/numpy/basic/numpy-basic-exercise-41.php)\n",
    "\n",
    "2. Time Series\n",
    "\n",
    "    [wikipedia: Time Series](https://en.wikipedia.org/wiki/Time_series#:~:text=In%20mathematics%2C%20a%20time%20series,equally%20spaced%20points%20in%20time.)\n",
    "\n",
    "    [Stack Exchange: features for time series classification](https://stats.stackexchange.com/questions/50807/features-for-time-series-classification)\n",
    "\n",
    "    [wikipedia: Time Domain](https://en.wikipedia.org/wiki/Time_domain#:~:text=Time%20domain%20refers%20to%20the,data%2C%20with%20respect%20to%20time.)\n",
    "\n",
    "    [Design of the Second-Order Controller by Time-Domain Objective Functions Using Cuckoo Search  DOI: 10.5772/intechopen.89832](https://www.intechopen.com/chapters/69848)\n",
    "\n",
    "    [Time Series Forecasting with Machine Learning - youtube.com](https://www.youtube.com/watch?v=_ZQ-lQrK9Rg)\n",
    "\n",
    "3.  [Calculating Confidence Intervals with Bootstrapping](https://towardsdatascience.com/calculating-confidence-interval-with-bootstrapping-872c657c058d)\n",
    "\n",
    "4. AI: claudi.ai - chat screenshot [here](./Chat_link.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
